#+title: Heap

* Info
A heap is a tree to help tracking min/max nodes.

Regular sorted trees divide their data into K bucketed and ordered sub-trees.
Heaps do the same with a more literal heap of data (buckets have no structure
other than being worse than their parent. All K sub-heaps look similar) that
also makes finding the min/max quick.

* Bunch of heaps
** Binary heap
#+begin_quote
                             0
                1                         2
         3            4            5             6
     7      8      9     10    11     12     13     14
   15 16  17 18  19 20  21 22 23 24  25
#+end_quote

*** Index jumps
- Up: (i-1)//2
- DL: (2*i) + 1
- DR: 2(i+1)

** Ternary heap
#+begin_quote
                                                     0
                   1                                 2                                 3
      (4           5          6)        (7           8          9)        (10          11         12)
  (14 15 16)  (17 18 19) (20 21 22) (23 24  -)
#+end_quote

*** Index jumps
- Up: (i-1)//3
- D1: (3*i) + 1
- D2: (3*i) + 2
- D3: (3*i) + 3 = 3(i+1)

** Quad-heap
#+begin_quote
                                                                                    0
                     (1                                        2                                      3                                           4)
       (5         6       7       8)             (9       10      11       12)             (13       14      15       16)                (17       18      19       20)
(21 22 23 24) (25  -)
#+end_quote

*** Index jumps
- Up: (i-1)//4
- D1: (4*i) + 1
- D2: (4*i) + 2
- D3: (4*i) + 3
- D4: (4*i) + 4 = 4(i+1)

* Operations
** External
*** Peek min/max
- Cost :: O(1)
*** Pop
- Cost :: O(log_K(N))
  - Comparisons :: log_k(N)..k log_k(N)
  - Swaps :: log_k(N)
  - Pop :: O(1)
**** Notes
***** Deferred sync
When popping you could return in O(1) and perform the removal later.
- If you pop/push, then pushing can start with a sift-down from the hole at the top.
- If you pop/pop, you should just do the work.
  - You can start cheating and peeking deeper into the tree while keeping
    shortlist of holes, but it's probably not worth it.
*** Push
- Cost :: O(log_K(N))
  - Appending :: â‰…O(1), but may trigger ~Vec<T>~ re-allocation.
  - Comparisons :: O(log_k(N));  log_k(N)..k log_k(N)
  - Swaps :: O(log_k(N)); 0..log_k(N)

** Internal
*** Vector append / pop
*** Parent-Child swapping
*** Sift up/down

* Memory access
*** Backing store
Out of object-oriented 101 class it's tempting to write something along,

#+begin_src python
  class BinaryHeapNode:
    def __init__(self, value):
      self.value = value  # This could be a reference to actual data.
      self.left = None
      self.right = None
#+end_src

However, this leads to code that jumps around a lot. When describing the heaps
we also numbered them, which means that it's not hard to use a ~Vec<T>~ to
maintain the heap.

*** Node data
When designing the node it's tempting to save data and use the naturally minimal
pointer ~*T~ even a ~u32~ or a ~u16~ short vector index if we have less than
~4'294'967'295~ or ~65'535~ elements.

Doing this results in having to jump to the backing arena/vector to be able to
compare elements, but it's probably worth de-normalising the data by duplicating
the part of the node that's needed to run the comparisons.

If in order to run each comparison we need to access ~*entity~ we read a lot of
data that corresponds to ~Entity~.

#+begin_src rust
  struct Entity {
    bunch_of_irrelevant_stuff_when_ranking: [u8; 40],
    power_level: u16,
  }
#+end_src

It'd be better to bundle the ranking key in our heap,

#+begin_src rust
  struct Entity {
    fn rank(&self) -> EntityRank;
  }
  struct EntityRank {
    power_level: u16,
  }
#+end_src

And use a ~(RankingKey, &Entity)~ pair in our heap. Don't be fooled by the need
of keeping things in sync. If the Entity were to change outside of the Heap, you
are equally in trouble as the heap's ranking would be off.

*** Changing values
When the entities you rank can change, you need a way of signalling the heap that
it needs to consider that.

One trivial way is that after a change to an entity, you could look at the heaps
that refer to it, and in each one run a linear search to find the heap node that
needs updating.

This isn't great, what if the heaps could be proactive and keep the Entities
aware of where they are in each heap? This idea leads to Intrusive
Data-structures a class of Data Structures that writes data beyond their owned
memory to allow optimising across data structures.
Rust really hates intrusive data structures, but they allow O(1) access here in
a way that's just not possible without wanting to delve into the world of not so
encapsulated data.

*** Path access
Let's look at a binary tree. What memory regions are needed when operating the tree?

**** Example
#+begin_quote
                               ~0~
                ~1~                             2
       3                ~4~               5            6
  7        8        ~9~      10      11     12     13     14
15 16    17 18    ~19~ 20  21  22  23  24  25 26  27 28  29 30
                ~39~ (40)
#+end_quote

***** Downwards (pop)
Let's say we pop a node leaving a hole at ~@0~ that bubbles-up ~@39~

Downward path ~0 1 4 9 19 39~. When comparing we read these nodes,
#+begin_quote
 ~00~  01  02  03 04  05 06 07 08  09 10  11 12 13 14 15 16 17 18  19 20  21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  39 40  41 42 43 44 45
  00  ~01 02~  03 04  05 06 07 08  09 10  11 12 13 14 15 16 17 18  19 20  21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  39 40  41 42 43 44 45
  00   01  02 ~03 04~ 05 06 07 08  09 10  11 12 13 14 15 16 17 18  19 20  21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  39 40  41 42 43 44 45
  00   01  02  03 04  05 06 07 08 ~09 10~ 11 12 13 14 15 16 17 18  19 20  21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  39 40  41 42 43 44 45
  00   01  02  03 04  05 06 07 08  09 10  11 12 13 14 15 16 17 18 ~19 20~ 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  39 40  41 42 43 44 45
  00   01  02  03 04  05 06 07 08  09 10  11 12 13 14 15 16 17 18  19 20  21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 ~39 40~ 41 42 43 44 45
#+end_quote

***** Upwards (push)
Let's say we append at ~@39~. This new node may sift up until the very top.

We could swap along this path ~40 19 9 4 1 0~, but bailing out ASAP.

#+begin_quote
  00   01  02 03  04  05 06 07 08  09  10 11 12 13 14 15 16 17 18  19  20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 38 ~39~
  00   01  02 03  04  05 06 07 08  09  10 11 12 13 14 15 16 17 18 ~19~ 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 38  39
  00   01  02 03  04  05 06 07 08 ~09~ 10 11 12 13 14 15 16 17 18  19  20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 38  39
  00   01  02 03 ~04~ 05 06 07 08  09  10 11 12 13 14 15 16 17 18  19  20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 38  39
  00  ~01~ 02 03  04  05 06 07 08  09  10 11 12 13 14 15 16 17 18  19  20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 38  39
 ~00~  01  02 03  04  05 06 07 08  09  10 11 12 13 14 15 16 17 18  19  20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 38  39
#+end_quote
**** What about it?
Computer memory does not work in nodes terms, but _cache-lines_, meaning that
the computer may end up loading more memory than what's precisely needed if the
batch sized used for each memory operation is bigger than what we use.

We could try using wider heaps such that the entire cache-line retrieved and
stored is useful. Also, reading multiple sequential cache-lines is faster than
jumping around and allows for ~SIMD~ to shine, so 512-bit (64B) wide levels
don't sound so wide anymore, jumping around is the scary thing here.

We also fall in the world of sorting networks.
- Filling a hole during ~pop()~ requires selecting the min of K contiguous
  elements.
- Bubbling up a node during ~push()~ would be easy if we could quickly sort K
  contiguous elements. Good asymptotic performance isn't necessary here.

*** Actual usage
Heaps are great, but what if your key diversity was low? Maybe working around a
~HashMap<K, Vec<T>>~ would ensure inserting/removing is fast.

Some 2D 4-connected search problems lead to f-values being in the [k, k+3] range
at all times! While this may sound surprising and a rare occurrence, this is not
that unexpected,
- When searching you only explore nodes with minimal f-value
- If costs are bounded, then you can only reach ~minimal-f-value + MaxCost~ nodes.
- If costs are integral, then there's a countable amount of possible ~f-values~
  in your heap.
